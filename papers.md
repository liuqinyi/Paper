---
fignos-cleveref: On
fignos-plus-name: 图
---

#  基于scene graph生成真实图像
## 摘要
图像生成一直是计算机视觉领域的一个巨大挑战，创建一个好的图像生成模型面临诸多困难。现存的一些图像生成模型，可以实现文本-图像，随机噪声-图像，以及图像-图像。但是生成图像都存在一定的不足，
通过自然语言描述生成图像的技术已经有实现，这些方法再某些特定的领域取得相当好的效果，比如鸟或花的描述，但是当用复杂的文本序列来描述多对象和多关系的时候就会变得非常乏力。为了解决这些限制性问题我们推出一种从scene graph来生成高分辨率图像。

## 1 引言
随着深度学习的巨大发展，越来越多的人工智能领域问题运用深度学习的方式来解决。图像生成是计算机视觉领域一项巨大的挑战，其关键思想就是找到一个低维的表示潜在空间(latent space)，其中任意一点可以映射为有一张逼真的图像，由于深度学习可以较好地利用反馈信息对模型进行调整，因此运用深度学习可以通过学习的方式得到理想的图像生成模型。

基于深度学习思想的三大生成模型：变分自编码器(Variational autoencoder)、生成式对抗网络(Generative Adversarial Network)、自回归方法(autoregressive approaches)。变分自编码器(VAE)是通过变分推理的方式联合地学习一对 *编码器(encoder)* 和 *解码器(decoder)* 组，实现真实图像与潜在空间(latent space)的映射关系。生成式对抗网络(GANs)联合地学习一对 *生成器(G)* 和 *判别器(D)*，生成器通过随机噪声生成假图像，而判别器则判断输入为生成的假图像还是还是真实数据集。自回归方法是依据前面的所有像素预判下一个像素的生成。{@fig:vae}

![VAE](assets/VAE.png){#fig:vae}
![GAN](assets/GAN.png){#fig:gan}

近期由于GANs网络的火热，学术界提出大量GANs的变种,比如条件生成对抗网络(CGAN)、信息生成对抗网络(InfoGAN)以及深度卷积对抗网络(DCGAN)等，这些模型为图像生成做出了巨大贡献。Scott Reed等[@]运用CGAN模型实现基于文本描述生成与文本语义相关的图像。但是这个模型并不完美，如果简单地调整输出图形的大小来生成高分辨率图像时，就会产生很多无意义的输出。因此Han zhang等[@]在此基础上提出StackGAN模型，它有两个GAN模型。第一个GAN输入文本序列，输出包含图像大致形状和颜色；第二个GAN以第一个GAN的生成的低分辨率图像和嵌入式文本向量作为输入，高分辨率图像作为输出。这个模型已经能生成照片般真实的图像，但是对于对于关系复杂的图像的生成并不很友好，当描述文本包含多个对象和多种关系的时候生成图像就会显得不够真实。因此Justin Johnson等[@]讲文本序列替换成scene graph作为输入，通过一个graph convoluition network沿着graph的边进行处理，将获取的信息用于预测一个scene layout,并输入到一个 *级联细化网络(cascaded refinement network)* 来生成图像。

为使生成图像更加真实，图像中的对象包含更多细节，对象之间的关系更加明确。本文将生成scene layout 再次映射到潜在空间，并将潜在向量作为GAN中生成器的输入，生成图像和真实图像作为判别器的输入对抗式训练。此时生成图像包含复杂场景图像中的各个对象以及它们之间的大致关系，为使这些对象和对象之间关系包含更多细节，本文训练一个新的GAN网络，将前一阶段的生成图像以及scene graph卷积向量作为输入，以输出像照片一般真实的高分辨的复杂场景图像。

## 2 相关研究背景
图像生成模型是计算机视觉领域的一个基本问题，随着深度学习的出现，这方面取得了显著进步。而生成式对抗网络(GAN)在生成清晰图像方面展现了非常强势的一面，它的一个变种条件生成对抗网络(CGAN)使用最为广泛。

条件图像生成模型常用一些额外简单条件替换随机变量作为生成器或者判别器的输入，比如Xinchen Yan等[@]将视觉属性作为GAN模型的输入来生成图像；Odena等[@]实现以类别标签作为条件变量生成图像；Scott Reed等[@]通过将文本序列转换为词向量作为条件输入，实现了 *text-to-image* 模型。当然还有很多是将图像作为条件生成图像的，包括：超分辨率重建(SRGAN)[@]、照片编辑[@]、域转换[@]。

由于文本序列一个单词接着一个单词的线性结构，很难表征一副具有多个对象和复杂关系的图像，因此本文引入scene graph作为GAN模型的条件变量，如图所示{fig：}，相对自然语言文本而言，它更能清楚地表示对象以及它们之间的关系，scene graph更像是自然语言和图像之间的过渡桥梁。同时，scene graph还广泛应用于图像语义检索[@]、评估和改进图像标题[@]。Schuster等[@]实现将文本描述转换为scene graph,以及从图像中预测scene graph[@]。

为了生成一张能尽可能表征scene graph的图像，就需要对scene graph尽心前期处理，将graph中的对象和对象之间的关系转换为向量。为此本文采用图卷积网络(graph convolution network)[@],沿着graph的边传递信息。然后我们需要根据获得的信息向量构建一个图布局，这个图布局包括途中所有对象的边界框和分割掩模。运用级联细化网络慢慢提升图布局的空间尺度以得到最终生成图像。

除了使用单一的GAN模型来生成图像，还可以利用多个GAN来进行图像生成操作[@]。Han zhang等[@]提出使用两个阶段来生成图像，第二阶段旨在增强细节。但是它只适用于简单图像，比如鸟类或者花类图像的生成。本文采用第二阶段的GAN不仅可以细化对象，并且能清晰增强对象之间的关系，生成更假真实的图像。

## 3  网络模型结构
为了生成高分辨率的复杂场景图像，需要建立这样一个模型：输入一张scene graph，输出一张高分辨率复杂场景图。其中主要的挑战有三点：1. 创建一个处理有向图输入的方法；2. 保证生成图像能清晰表示从场景图中解析出来的对象和关系；3. 必须保证合成图像足够的真实。

使用如下公式表示从场景图到真是图像的转换,其中输入$g$表示场景图，$z$表示噪声，$I$为输出图像
$$
I = f(g,z)
$$

通过一个GNNs来将$g$中的所有对象转换为嵌入向量 **(embedding vector)** ，有向图卷积的每一场沿着有向图的边混合信息，如图{fig:}。然后利用embedding vector 预测每个对象的bounding boxes和segementation mask，这些bounding boxes和segementation mask组合就能构成一个**scene layout**，可以充当有向图和图像之间的中间物。

将 **scene layout** Downsamples之后作为一个级联细化网络(CRNs)的输入，附上噪声做卷积就可生成一张图像作为输出。

为了保证生成图像更加真实并包含真实、可识别的对象，还需通过一对鉴别器网络$D_{img}$和$D_{obj}$对抗训练。每一个模型组件我们再后面都会做更详细介绍。

### 2.1 scene graph
 一个场景图可以表示为一个元组$(O,E)$,其中$O={\{o_1,o_2,\dots o_n \}}$是对象集，表示有向图中的节点或者图像中的对象，$o_i\in C$，$C$是所有对象类别的一个集合。$E={\{(o_i,r,o_j)\}}$，其中$o_i, o_j\in C, r \in R$, 集合$R$表示所有关系类别，即有向图中的边。

作为第一步处理，通过学习得到一个嵌入层可以将有向图中的节点和边都转换为密集型向量，类似于自然语言处理中的词嵌入层。
### 2.2 scene graph convolution network
为了能以端对段的方式处理场景图，模型设计一个由几个场景图卷基层组成的场景图卷积网络。一个传统的
### 2.3 scene layout
### 2.4 级联细化网络
### 2.5 鉴别器网络
### 2.6 训练


## 3 实验和结果分析
### 3.1 实验数据集和评估方法
#### COCO
#### Visual Genome
####　评估方法
### 3.2 实验结果

## 4 总结
