---
fignos-cleveref: On
fignos-plus-name: 图
---

#  基于Attention GCN 技术的图像生成
## 摘要
图像生成一直是计算机视觉领域的一个巨大挑战，为了生成更加真实的图像，图像生成模型需要考虑诸多方面的影响。条件生成式对抗网络(CGAN)广泛用于图像生成，包括text-to-image、image-to-image。但是这些模型的只是对于单场景图像的生成比较友好，因此为了生成更加复杂的多场景图像，本文引入一种graph-to-image模型。该模型主要优点在于模型输入为graph，而graph是一种可以很好地表征图像中的对象以及它们之间的关系的数据结构。Scott等使用一种图卷积网路(GCA)对 输入graph进行处理以实现端对端模型，但是这种方式默认所有的对象都是平等的关系。为了考虑不同对象之间的主次关系，本文在图卷积中引入一种注意力机制，实现不同对象之间的主次、远近、方位等关系属性。本文在COCO数据集上对本文方法进行实验， 证实本文方法对于生成图像中的对象有更好的区分度。

## 1 引言
随着深度学习的发展，运用深度学习的方式实现图像生成逐渐取代传统的图像处理中的方法。较为流行的三种生成模型分别为生成式对抗网络（GAN）、变分自编码器（VAE）以及自回归模型（Autoregressive Models）。

近期大部分图像生成模型都是基于生成式对抗网络（GAN）或其变体实现，其核心思想是通过条件输入和真实标签联合的训练一对 *生成器（Genertor）* 和 *判别器(Discriminator)* 。其中将自然语言描述作为条件输入自动生成图像是一个基本问题，其主要解决方法是将文本描述编码为句子向量。在这方面实现效果较好的是Scott等提出的DCGAN模型，其实验精度达到0.76。而为了解决生成高分辨率图像而无法训练的问题，Scott又提出StackGAN模型，主要是通过多种GAN模型叠加训练逐步提升生成图像分辨率，其分别在COCO和CUBS数据集上训练，得到最好的结果为—————。当然为了让生成图像包含更多的细节，XuTao等在StacKGAN的基础上引入Attention机制，增加深度注意多模态相似模型。其主要思想是将最后一个GAN的生成器生成图像编码成特征子图，并和描述文本中的单词向量进行匹配求相似度，以实现描述文本中的单词与生成图像中的子域对应生成。该方法在与前面的方法相比精度提高了————。

虽然这些方法能实现高分辨率图像的生成，但是这仅限于对象较少的单场景图生成，对于包含多对象和关系的复杂场景图生成就会出现————的情况。为了解决这个问题，Scott等提出graph-to-image的生成模型，其系统流程图如图所示。该模型主要用graph作为条件输入，因graph具有————的结构，因此在表示图像中对象关系上相对于文本描述有更好的效果。且近期由于一些列的图神经网络（GNNs）的提出，解决了graph数据的————弊端，增加了graph训练的可行性。

该模型采用非谱域的图卷积网络(GCNs)实现graph的处理，这是一种沿边传播信息的网络模型，其主要应用于————等领域。这种模型得到的向量只包含边相连节点的信息，这对于生成图像来说显得信息量不够，因为图中每个对象之间都有一定比重的关系。因此为了使得每个节点包含所有其他节点的信息，本文提出一种基于空间谱域的Attention GCNs网络模型来对graph进行处理，具体细节会在后面章节介绍。

为了合成高分辨率的图像，本文将采用逆卷积网络实现布局图到图像的生成。并且为了让最后生成的图像中的对象能真实的表征graph中的中的节点，采用卷积的方式将其转换为和graph节点向量维度相同特征图，通过求相似度的方式来评估模型。最后，本文的模型在 *COCO-stuff* 数据集上进行训练，通过定量的数据比较，保证我们的模型在图像分辨率和图像细节的生成方面有着较大提升。

## 2 相关研究背景



图像生成模型是计算机视觉领域的一个基本问题，随着深度学习的出现，这方面取得了显著进步。而生成式对抗网络(GAN)在生成清晰图像方面展现了非常强势的一面，它的一个变种条件生成对抗网络(CGAN)使用最为广泛。

条件图像生成模型常用一些额外简单条件替换随机变量作为生成器或者判别器的输入，比如Xinchen Yan等[@]将视觉属性作为GAN模型的输入来生成图像；Odena等[@]实现以类别标签作为条件变量生成图像；Scott Reed等[@]通过将文本序列转换为词向量作为条件输入，实现了 *text-to-image* 模型。当然还有很多是将图像作为条件生成图像的，包括：超分辨率重建(SRGAN)[@]、照片编辑[@]、域转换[@]。

由于文本序列一个单词接着一个单词的线性结构，很难表征一副具有多个对象和复杂关系的图像，因此本文引入scene graph作为GAN模型的条件变量，如图所示{fig：}，相对自然语言文本而言，它更能清楚地表示对象以及它们之间的关系，scene graph更像是自然语言和图像之间的过渡桥梁。同时，scene graph还广泛应用于图像语义检索[@]、评估和改进图像标题[@]。Schuster等[@]实现将文本描述转换为scene graph,以及从图像中预测scene graph[@]。

为了生成一张能尽可能表征scene graph的图像，就需要对scene graph尽心前期处理，将graph中的对象和对象之间的关系转换为向量。为此本文采用图卷积网络(graph convolution network)[@],沿着graph的边传递信息。然后我们需要根据获得的信息向量构建一个图布局，这个图布局包括途中所有对象的边界框和分割掩模。运用级联细化网络慢慢提升图布局的空间尺度以得到最终生成图像。

除了使用单一的GAN模型来生成图像，还可以利用多个GAN来进行图像生成操作[@]。Han zhang等[@]提出使用两个阶段来生成图像，第二阶段旨在增强细节。但是它只适用于简单图像，比如鸟类或者花类图像的生成。本文采用第二阶段的GAN不仅可以细化对象，并且能清晰增强对象之间的关系，生成更假真实的图像。

## 3  网络模型结构
如图所示，本文模型主要有如下部分组成：Attention GCN用于处理输入图像，得到包含相邻节点信息的节点向量；Object Layout Net是根据新的节点向量生成Scene Layout； Cascaded Refinement Network通过一系列卷积细化模块合成高度表征对象布局的图像。


使用如下公式表示graph-to-iamge模型,其中输入$g$表示场景图，$z$表示噪声，$I$为输出图像。
$$
I = f(g,z)
$$
$f$即为目标所求，我们需要保证得到的函数f使得生成图像$I$和真实图像$\hat{I}$尽可能的相似，即：
$$
\min L= \{I, \hat{I}\}
$$

### 3.1 scene graph
 本文的模型输入是一个描述对象和对象之间关系的场景图。给定一个对象类别集$C$和一个关系类别集$R$，其中一个元组$(O,E)$就表示一张场景图。其中$O={\{o_1,o_2,\dots o_n \}}$表示图中所有的节点，$o_i\in C$，$n$为节点数；$E\subseteq O \times R \times O$是由${(o_i, r, o_j)}$形式组成的有向边的集合，其中$o_i, o_j\in C, r \in R$。

作为第一步处理，通过学习得到一个嵌入层可以将有向图中的节点和边都转换为密集型向量，类似于自然语言处理中的词嵌入层。
### 3.2 Attention GCN
相对于传统的image和text等规则的数据结构，graph的不规则性意味着其可以表示更丰富的信息。场景图的节点表示网络中的个体，边表示个体之间的关系。为了整合由图结构提供的上下文信息，以端对端的方式处理图，提出图卷积模型对输入图进行处理。

在GCN中，每个节点$o_i$有一个表示特征向量$h_i \in \Re^{D_{in}}$，$D_{in}$表示输入图的节点和边的特征向量的维度。首先通过训练学习得到线性变换矩阵$W$, 变换图中每个节点的相邻节点$\{h_j|j \in N(j)\}$。然后用预先设定的权重 $\alpha$ 收集这些变换,接着输入至非线性函数$\sigma$(ReLU)中。这种逐层变换可以写作：

$$
h_i^{(l+1)}=\sigma(h_i^{(l)} + \sum \alpha_{ij}W h_i^{(l)})
$$

或者可以等价地将节点表示收集至矩阵$H \in \Re^{d \times Tn}$，

$$
h_i^{(l+1)}=\sigma(WZ^{(l)} \alpha_i )
$$

在传统的GCN中，图中的连接是已知的，并且条件系数 $\alpha_i$ 是基于特征向量的对称归一化领接矩阵预先设定的。

在本文中，我们额外加入注意力机制，通过训练学习的方式调节条件系数 $\alpha$。为了从相邻节点特征中预测注意力，我们训练一个具有连续节点特征的两层 **MLP**，并再得到的结果分数上计算softmax。节点$o_j$的注意力计算公式如下：

$$
u_{ij}=w_h^T \sigma(W_a[h_i^{(l)}, h_j^{(l)}]) \\
\alpha_i = softmax(u_i)
$$

其中，$w_h$和$W_a$是通过学习得到的参数，$[\cdot , \cdot]$表示连接操作。根据定义，我们设置 $\alpha_{ii}=1$ 及 $\alpha_{ij}=0 \forall j \notin N(j)$。由于注意力是节点特征的函数，因此每次迭代都会导致注意力发生变化，从而影响连续迭代。
### 3.3 Object Layout Network
Attention GCN 通过一系列图像卷积处理输入场景图为每个对象提供嵌入式向量，该向量聚合图中所有对象和关系的信息。

为了生成图像，我们必须从图域移动到图像域。 为此，我们使用对象嵌入向量来计算场景布局，该场景布局给出了要生成的图像的粗糙2D结构; 我们通过使用对象布局网络为每个对象预测分段掩码和边界框来计算场景布局，如图4所示。

对象布局网络接收一个维度为$D$的表示结点对象$o_i$的嵌入向量$h_i$,并将其通过 *掩模回归网络* 来预测一个形状为$M \times M$的二进制掩模 $\hat m_i$以及通过 *框回归网络* 来预测一个边界框 $\hat b = (x_0, y_0, x_1, y_1)$ 。掩模回归网络由几个以S形非线性终止的转置卷积组成，因此掩模的元素位于范围$(0,1)$中; 框回归网络是**MLP**层。

将嵌入式向量好$h_i$和掩模相乘得到形状为$D \times M \times M$的掩模嵌入，然后用双线性插值法将其扭曲到边界框的位置以给出对象布局。然后场景布局是所有对象布局的总和。

在训练过程中，我们使用真实的边界框 $b_i$ 来计算场景布局; 在测试时，我们改为使用预测的边界框 $\hat b_i$。
### 3.4 Cascaded Refinement Network
给定场景布局，我们必须合成一个尊重布局中给出的对象位置的图像。 对于此任务，我们使用级联细化网络[6]（CRN）。 CRN由一系列卷积细化模块组成，模块之间的空间分辨率加倍; 这允许生成以粗到细的方式进行。

每个模块接收场景布局（下采样到模块的输入分辨率）和前一模块的输出作为输入。这些输入通道连接并传递给一对3×3卷积层; 然后，在传递到下一个模块之前，使用最近邻插值对输出进行上采样。

第一个模块将高斯噪声$z \sim p_z$作为输入，最后一个模块的输出传递给两个最终的卷积层以产生输出图像。

### 3.5 鉴别器网络
为了保证生成图像更加真实并包含真实、可识别的对象，还需通过一对鉴别器网络$D_{img}$和$D_{obj}$对抗训练。每一个模型组件我们再后面都会做更详细介绍。
### 2.6 训练


## 3 实验和结果分析
### 3.1 实验数据集和评估方法
#### COCO
#### Visual Genome
####　评估方法
### 3.2 实验结果

## 4 总结
