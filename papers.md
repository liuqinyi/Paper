---
fignos-cleveref: On
fignos-plus-name: 图
---

#  基于Attention GCN 技术的图像生成
## 摘要
图像生成一直是计算机视觉领域的一个巨大挑战，为了生成更加真实的图像，图像生成模型需要考虑诸多方面的影响。条件生成式对抗网络(CGAN)广泛用于图像生成，包括text-to-image、image-to-image。但是这些模型的只是对于单场景图像的生成比较友好，因此为了生成更加复杂的多场景图像，本文引入一种graph-to-image模型。该模型主要优点在于模型输入为graph，而graph是一种可以很好地表征图像中的对象以及它们之间的关系的数据结构。Scott等使用一种图卷积网路(GCA)对 输入graph进行处理以实现端对端模型，但是这种方式默认所有的对象都是平等的关系。为了考虑不同对象之间的主次关系，本文在图卷积中引入一种注意力机制，实现不同对象之间的主次、远近、方位等关系属性。本文在COCO数据集上对本文方法进行实验， 证实本文方法对于生成图像中的对象有更好的区分度。

## 1 引言
随着深度学习的巨大发展，越来越多的人工智能领域问题运用深度学习的方式来解决。图像生成是计算机视觉领域一项巨大的挑战，其关键思想就是找到一个低维的表示潜在空间(latent space)，其中任意一点可以映射为有一张逼真的图像，由于深度学习可以较好地利用反馈信息对模型进行调整，因此运用深度学习可以通过学习的方式得到理想的图像生成模型。

基于深度学习思想的三大生成模型：变分自编码器(Variational autoencoder)、生成式对抗网络(Generative Adversarial Network)、自回归方法(autoregressive approaches)。变分自编码器(VAE)是通过变分推理的方式联合地学习一对 *编码器(encoder)* 和 *解码器(decoder)* 组，实现真实图像与潜在空间(latent space)的映射关系。生成式对抗网络(GANs)联合地学习一对 *生成器(G)* 和 *判别器(D)*，生成器通过随机噪声生成假图像，而判别器则判断输入为生成的假图像还是还是真实数据集。自回归方法是依据前面的所有像素预判下一个像素的生成。{@fig:vae}

![VAE](assets/VAE.png){#fig:vae}
![GAN](assets/GAN.png){#fig:gan}

近期由于GANs网络的火热，学术界提出大量GANs的变种,比如条件生成对抗网络(CGAN)、信息生成对抗网络(InfoGAN)以及深度卷积对抗网络(DCGAN)等，这些模型为图像生成做出了巨大贡献。Scott Reed等[@]运用CGAN模型实现基于文本描述生成与文本语义相关的图像。但是这个模型并不完美，如果简单地调整输出图形的大小来生成高分辨率图像时，就会产生很多无意义的输出。因此Han zhang等[@]在此基础上提出StackGAN模型，它有两个GAN模型。第一个GAN输入文本序列，输出包含图像大致形状和颜色；第二个GAN以第一个GAN的生成的低分辨率图像和嵌入式文本向量作为输入，高分辨率图像作为输出。这个模型已经能生成照片般真实的图像，但是对于对于关系复杂的图像的生成并不很友好，当描述文本包含多个对象和多种关系的时候生成图像就会显得不够真实。因此Justin Johnson等[@]讲文本序列替换成scene graph作为输入，通过一个graph convoluition network沿着graph的边进行处理，将获取的信息用于预测一个scene layout,并输入到一个 *级联细化网络(cascaded refinement network)* 来生成图像。

为使生成图像更加真实，图像中的对象包含更多细节，对象之间的关系更加明确。本文将生成scene layout 再次映射到潜在空间，并将潜在向量作为GAN中生成器的输入，生成图像和真实图像作为判别器的输入对抗式训练。此时生成图像包含复杂场景图像中的各个对象以及它们之间的大致关系，为使这些对象和对象之间关系包含更多细节，本文训练一个新的GAN网络，将前一阶段的生成图像以及scene graph卷积向量作为输入，以输出像照片一般真实的高分辨的复杂场景图像。

## 2 相关研究背景
图像生成模型是计算机视觉领域的一个基本问题，随着深度学习的出现，这方面取得了显著进步。而生成式对抗网络(GAN)在生成清晰图像方面展现了非常强势的一面，它的一个变种条件生成对抗网络(CGAN)使用最为广泛。

条件图像生成模型常用一些额外简单条件替换随机变量作为生成器或者判别器的输入，比如Xinchen Yan等[@]将视觉属性作为GAN模型的输入来生成图像；Odena等[@]实现以类别标签作为条件变量生成图像；Scott Reed等[@]通过将文本序列转换为词向量作为条件输入，实现了 *text-to-image* 模型。当然还有很多是将图像作为条件生成图像的，包括：超分辨率重建(SRGAN)[@]、照片编辑[@]、域转换[@]。

由于文本序列一个单词接着一个单词的线性结构，很难表征一副具有多个对象和复杂关系的图像，因此本文引入scene graph作为GAN模型的条件变量，如图所示{fig：}，相对自然语言文本而言，它更能清楚地表示对象以及它们之间的关系，scene graph更像是自然语言和图像之间的过渡桥梁。同时，scene graph还广泛应用于图像语义检索[@]、评估和改进图像标题[@]。Schuster等[@]实现将文本描述转换为scene graph,以及从图像中预测scene graph[@]。

为了生成一张能尽可能表征scene graph的图像，就需要对scene graph尽心前期处理，将graph中的对象和对象之间的关系转换为向量。为此本文采用图卷积网络(graph convolution network)[@],沿着graph的边传递信息。然后我们需要根据获得的信息向量构建一个图布局，这个图布局包括途中所有对象的边界框和分割掩模。运用级联细化网络慢慢提升图布局的空间尺度以得到最终生成图像。

除了使用单一的GAN模型来生成图像，还可以利用多个GAN来进行图像生成操作[@]。Han zhang等[@]提出使用两个阶段来生成图像，第二阶段旨在增强细节。但是它只适用于简单图像，比如鸟类或者花类图像的生成。本文采用第二阶段的GAN不仅可以细化对象，并且能清晰增强对象之间的关系，生成更假真实的图像。

## 3  网络模型结构
如图所示，本文模型主要有如下部分组成：Attention GCN用于处理输入图像，得到包含相邻节点信息的节点向量；Object Layout Net是根据新的节点向量生成Scene Layout； Cascaded Refinement Network通过一系列卷积细化模块合成高度表征对象布局的图像。


使用如下公式表示graph-to-iamge模型,其中输入$g$表示场景图，$z$表示噪声，$I$为输出图像。
$$
I = f(g,z)
$$
$f$即为目标所求，我们需要保证得到的函数f使得生成图像$I$和真实图像$\hat{I}$尽可能的相似，即：
$$
\min L= \{I, \hat{I}\}
$$

### 3.1 scene graph
 本文的模型输入是一个描述对象和对象之间关系的场景图。给定一个对象类别集$C$和一个关系类别集$R$，其中一个元组$(O,E)$就表示一张场景图。其中$O={\{o_1,o_2,\dots o_n \}}$表示图中所有的节点，$o_i\in C$，$n$为节点数；$E\subseteq O \times R \times O$是由${(o_i, r, o_j)}$形式组成的有向边的集合，其中$o_i, o_j\in C, r \in R$。

作为第一步处理，通过学习得到一个嵌入层可以将有向图中的节点和边都转换为密集型向量，类似于自然语言处理中的词嵌入层。
### 3.2 Attention GCN
为了整合由图结构提供的上下文信息，本文使用基于注意力机制的图卷积网络。在我们描述attention GCN之前，先回顾一下GCN的实现。
为了能以端对段的方式处理场景图，模型设计一个由几个场景图卷基层组成的场景图卷积网络。一个传统的
### 3.3 Object Layout Network

### 3.4 Cascaded Refinement Network

### 3.5 鉴别器网络
为了保证生成图像更加真实并包含真实、可识别的对象，还需通过一对鉴别器网络$D_{img}$和$D_{obj}$对抗训练。每一个模型组件我们再后面都会做更详细介绍。
### 2.6 训练


## 3 实验和结果分析
### 3.1 实验数据集和评估方法
#### COCO
#### Visual Genome
####　评估方法
### 3.2 实验结果

## 4 总结
